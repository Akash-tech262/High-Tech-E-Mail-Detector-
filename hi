# file: phishing_hitech.py
# pip install scikit-learn joblib pandas numpy transformers torch regex beautifulsoup4 tldextract
import re, os, time, joblib, math, warnings, json
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import tldextract
from typing import List, Dict, Any

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import classification_report, precision_recall_fscore_support, roc_auc_score

warnings.filterwarnings("ignore")

# -------------------- Adversarial-Aware Normalization --------------------
ZERO_WIDTH = r"[\u200b\u200c\u200d\u2060]"
HOMOGLYPHS = {
    "а":"a","е":"e","о":"o","р":"p","с":"c","х":"x","і":"i","ί":"i","Ι":"I","Ο":"O","Β":"B",
    "ϵ":"e","ί":"i","ⅼ":"l","Ｓ":"S","０":"0","１":"1","５":"5"
}
def normalize_text(x:str)->str:
    if not isinstance(x,str): x=str(x)
    s = re.sub(ZERO_WIDTH,"",x)                 # remove zero-width
    s = s.encode("utf-8","ignore").decode("utf-8")
    # unicode homoglyph map
    s = "".join(HOMOGLYPHS.get(ch, ch) for ch in s)
    # collapse spaces & lowercase
    s = re.sub(r"\s+"," ",s).strip().lower()
    return s

# Simple adversarial augmentations for training hardening
def adversarial_augs(text:str)->List[str]:
    t = normalize_text(text)
    augs = [t]
    # insert random dots in domain-like tokens
    augs.append(re.sub(r"(http[s]?://)(\w)", r"\1.\2", t))
    # replace o->0, i->1 in urls
    augs.append(re.sub(r"([a-z])", lambda m: {"o":"0","i":"1"}.get(m.group(1), m.group(1)), t))
    # add spaces around punctuation
    augs.append(re.sub(r"([:/._-])", r" \1 ", t))
    return list({x for x in augs if x})

# -------------------- Security Heuristic Features --------------------
SUSP_TLDS = {".ru",".xyz",".top",".click",".work",".zip",".kim",".cn",".gq",".tk",".ml",".cf"}
URGENT_WORDS = {"verify","urgent","update","password","suspend","limited","expire","confirm","now","immediately","login"}
BRAND_BAIT = {"microsoft","office365","netflix","amazon","apple","paypal","facebook","instagram","bank","upi","aadhaar"}

URL_RE = re.compile(r'https?://[^\s<>\)"]+')
def url_features(text:str)->Dict[str,float]:
    urls = URL_RE.findall(text)
    num_urls = len(urls)
    feats = dict(
        has_url = 1 if num_urls>0 else 0,
        num_urls = float(num_urls),
        max_url_len = float(max((len(u) for u in urls), default=0)),
        avg_url_len = float(np.mean([len(u) for u in urls]) if urls else 0.0),
        suspicious_tld = 0.0,
        has_ip_url = 0.0,
    )
    for u in urls:
        ext = tldextract.extract(u)
        tld = "." + (ext.suffix or "")
        if tld in SUSP_TLDS: feats["suspicious_tld"] = 1.0
        # IP in host?
        if re.search(r'://\d{1,3}(\.\d{1,3}){3}', u): feats["has_ip_url"] = 1.0
    return feats

def html_features(text:str)->Dict[str,float]:
    soup = BeautifulSoup(text, "html.parser")
    html_len = len(text)
    num_links = len(soup.find_all("a"))
    num_forms = len(soup.find_all("form"))
    num_inputs = len(soup.find_all("input"))
    has_onclick = 1.0 if "onclick" in text.lower() else 0.0
    return dict(
        html_len=float(html_len),
        num_links=float(num_links),
        num_forms=float(num_forms),
        num_inputs=float(num_inputs),
        has_onclick=has_onclick
    )

def lexical_features(text:str)->Dict[str,float]:
    t = normalize_text(text)
    words = re.findall(r"[a-z0-9]+", t)
    word_len = [len(w) for w in words] or [0]
    urgent = sum(t.count(w) for w in URGENT_WORDS)
    brand = sum(1 for w in BRAND_BAIT if w in t)
    digits = sum(c.isdigit() for c in t)
    return dict(
        num_tokens=float(len(words)),
        avg_token_len=float(np.mean(word_len)),
        num_digits=float(digits),
        urgent_hits=float(urgent),
        brand_bait=float(brand)
    )

SEC_KEYS = [
    "has_url","num_urls","max_url_len","avg_url_len","suspicious_tld","has_ip_url",
    "html_len","num_links","num_forms","num_inputs","has_onclick",
    "num_tokens","avg_token_len","num_digits","urgent_hits","brand_bait"
]
def security_feature_matrix(texts:List[str])->np.ndarray:
    rows=[]
    for t in texts:
        u = url_features(t)
        h = html_features(t)
        l = lexical_features(t)
        row = {**u, **h, **l}
        rows.append([row[k] for k in SEC_KEYS])
    return np.asarray(rows, dtype=float)

# -------------------- Transformer Embeddings --------------------
# Small, fast model for CLS embedding
from transformers import AutoTokenizer, AutoModel
class TransformerEncoder:
    def __init__(self, name:str="distilbert-base-uncased", max_len:int=256, device:str="cpu"):
        self.name = name
        self.tok = AutoTokenizer.from_pretrained(name)
        self.model = AutoModel.from_pretrained(name)
        self.model.eval()
        self.max_len = max_len
        self.device = device
        self.model.to(device)

    @torch.inference_mode()
    def encode(self, texts:List[str])->np.ndarray:
        import torch
        toks = self.tok(texts, padding=True, truncation=True, max_length=self.max_len, return_tensors="pt")
        toks = {k:v.to(self.device) for k,v in toks.items()}
        out = self.model(**toks)
        # Use [CLS] token or mean pooling if CLS not available
        if hasattr(out, "last_hidden_state"):
            x = out.last_hidden_state.mean(dim=1)  # mean pool
        else:
            x = out[0].mean(dim=1)
        return x.cpu().numpy().astype(np.float32)

# Lazy import torch to avoid top-level requirement errors in some envs
import importlib
torch = importlib.import_module("torch")

# -------------------- Vectorizers & Models --------------------
char_tfidf = TfidfVectorizer(
    analyzer="char",
    ngram_range=(3,5),      # robust to obfuscation
    min_df=1
)

# Base learners
base_linear_svc = LinearSVC(C=1.0, class_weight="balanced")
base_linear_svc_cal = CalibratedClassifierCV(base_linear_svc, method="sigmoid", cv=3)  # get proba

base_rf = RandomForestClassifier(n_estimators=400, max_depth=None, n_jobs=-1, class_weight="balanced")

meta_lr = LogisticRegression(max_iter=1000, class_weight="balanced")

# -------------------- End-to-End High-Tech Model --------------------
class HighTechPhish:
    def __init__(self, threshold:float=0.35, device:str="cpu"):
        """
        threshold: probability above which we mark as phishing (lower -> more sensitive)
        """
        self.threshold = threshold
        self.scaler = StandardScaler()
        self.encoder = TransformerEncoder(device=device)
        self.fitted = False

    def _prepare_views(self, X:List[str]):
        texts_norm = [normalize_text(x) for x in X]
        # View 1: char TF-IDF
        v1 = self.tfidf.transform(texts_norm)
        # View 2: security numeric
        sec = security_feature_matrix(X)
        v2 = self.scaler.transform(sec)
        # View 3: transformer
        v3 = self.encoder.encode(X)
        return v1, v2, v3

    def fit(self, X:List[str], y:List[int], augment:bool=True):
        # Build augmented training set
        X_aug, y_aug = [], []
        for t, label in zip(X, y):
            variants = adversarial_augs(t) if augment else [t]
            X_aug.extend(variants); y_aug.extend([label]*len(variants))

        # Fit char TF-IDF
        self.tfidf = char_tfidf.fit([normalize_text(x) for x in X_aug])
        # Fit security scaler
        sec = security_feature_matrix(X_aug)
        self.scaler.fit(sec)
        # Prepare views
        v1 = self.tfidf.transform([normalize_text(x) for x in X_aug])
        v2 = self.scaler.transform(sec)
        v3 = self.encoder.encode(X_aug)

        # Base models
        self.clf_tfidf = base_linear_svc_cal.fit(v1, y_aug)
        self.clf_sec   = base_rf.fit(v2, y_aug)
        # Reduce dimensionality for v3 via logistic (acts as base)
        self.clf_bert  = LogisticRegression(max_iter=1000, class_weight="balanced").fit(v3, y_aug)

        # Meta stacking features = probs from bases
        p1 = self.clf_tfidf.predict_proba(v1)[:,1]
        p2 = self.clf_sec.predict_proba(v2)[:,1]
        p3 = self.clf_bert.predict_proba(v3)[:,1]
        P = np.vstack([p1,p2,p3]).T
        self.meta = meta_lr.fit(P, y_aug)
        self.fitted = True
        return self

    def predict_proba(self, X:List[str])->np.ndarray:
        assert self.fitted, "Model not trained. Call fit() or load() first."
        v1, v2, v3 = self._prepare_views(X)
        p1 = self.clf_tfidf.predict_proba(v1)[:,1]
        p2 = self.clf_sec.predict_proba(v2)[:,1]
        p3 = self.clf_bert.predict_proba(v3)[:,1]
        P = np.vstack([p1,p2,p3]).T
        p = self.meta.predict_proba(P)[:,1]
        return p

    def predict(self, X:List[str])->np.ndarray:
        p = self.predict_proba(X)
        return (p >= self.threshold).astype(int)

    def explain(self, X:List[str], top_k:int=8)->List[Dict[str,Any]]:
        """
        Lightweight explanation: top char n-grams (coef), security feats, and brand/urgent hits.
        """
        out=[]
        for text in X:
            t = normalize_text(text)
            # TF-IDF top signals
            resp = {}
            try:
                vec = self.tfidf.transform([t])
                coefs = self.clf_tfidf.base_estimator.coef_.toarray()[0]
                # grab top positive n-grams present
                idx = vec.nonzero()[1]
                contrib = [(self.tfidf.get_feature_names_out()[i], float(coefs[i]*vec[0, i])) for i in idx]
                contrib_sorted = sorted(contrib, key=lambda z: z[1], reverse=True)[:top_k]
                resp["top_char_ngrams"] = contrib_sorted
            except Exception:
                resp["top_char_ngrams"] = []
            # security feats
            sec = {k:v for k,v in zip(SEC_KEYS, security_feature_matrix([text])[0].tolist())}
            resp["security_feats"] = {k: float(sec[k]) for k in SEC_KEYS}
            # brand/urgent extracted
            resp["urgent_hits"] = sum(t.count(w) for w in URGENT_WORDS)
            resp["brands_found"] = [b for b in BRAND_BAIT if b in t]
            out.append(resp)
        return out

    def save(self, path:str):
        os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
        blob = dict(
            threshold=self.threshold,
            tfidf=self.tfidf,
            scaler=self.scaler,
            clf_tfidf=self.clf_tfidf,
            clf_sec=self.clf_sec,
            clf_bert=self.clf_bert,
            meta=self.meta,
            encoder_name=self.encoder.name,
        )
        joblib.dump(blob, path)

    def load(self, path:str, device:str="cpu"):
        blob = joblib.load(path)
        self.threshold = blob["threshold"]
        self.tfidf = blob["tfidf"]
        self.scaler = blob["scaler"]
        self.clf_tfidf = blob["clf_tfidf"]
        self.clf_sec = blob["clf_sec"]
        self.clf_bert = blob["clf_bert"]
        self.meta = blob["meta"]
        self.encoder = TransformerEncoder(name=blob.get("encoder_name","distilbert-base-uncased"), device=device)
        self.fitted = True
        return self

# -------------------- Demo / Training --------------------
if __name__ == "__main__":
    # Tiny illustrative dataset (replace with your CSV)
    data = pd.DataFrame([
        {"text": "Verify your Microsoft account urgently: http://bad.link/login", "label": 1},
        {"text": "Team meeting at 3 PM. Agenda attached.", "label": 0},
        {"text": "Your package is on hold. Update payment now: http://suspicious.ru/pay", "label": 1},
        {"text": "Invoice for last month attached. Thank you.", "label": 0},
        {"text": '<html><body>Reset your password <a href="http://secure-update.xyz/reset">here</a></body></html>', "label": 1},
        {"text": "Dinner at 8? Bring the sweets.", "label": 0},
        {"text": "⚠ Your account will be suspended. Confirm now: http://203.10.10.5/secure", "label": 1},
        {"text": "Project status: all green. Next review Monday.", "label": 0},
    ])

    X = data["text"].tolist()
    y = data["label"].tolist()

    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.35, random_state=42, stratify=y)

    model = HighTechPhish(threshold=0.35, device="cpu")
    model.fit(X_tr, y_tr, augment=True)

    # Evaluate with calibrated probabilities
    probs = model.predict_proba(X_te)
    preds = (probs >= model.threshold).astype(int)
    print(classification_report(y_te, preds, digits=3))
    try:
        auc = roc_auc_score(y_te, probs)
        print("ROC-AUC:", round(auc, 3))
    except Exception:
        pass

    # Explain a couple of examples
    exps = model.explain(X_te[:2], top_k=6)
    print("EXPLANATIONS:", json.dumps(exps, indent=2))

    # Save
    model.save("models/phishing_hitech_v1.joblib")
    print("Saved -> models/phishing_hitech_v1.joblib")

# from phishing_hitech import HighTechPhish
import joblib
from phishing_hitech import HighTechPhish

# Load high-tech model
ht = HighTechPhish().load("models/phishing_hitech_v1.joblib")

def analyze_email():
    text = textbox.get("1.0", tk.END).strip()
    if not text:
        messagebox.showwarning("Input Error", "Please paste some email text!")
        return
    proba = float(ht.predict_proba([text])[0])
    prediction = int(proba >= ht.threshold)
    msg = f"Prob(phishing) = {proba:.2f}\n"
    if prediction == 1:
        result = "⚠️ Warning: PHISHING"
        color = "red"
    else:
        result = "✅ Looks LEGIT"
        color = "green"
    result_label.config(text=msg + result, fg=color)
file: app_streamlit.py

Streamlit GUI for High-Tech Phishing Detector (v3)

Features:

- Paste email text OR upload .eml/.txt OR batch CSV with a text column

- Loads high-tech model (phishing_hitech_v1.joblib), or trains a tiny fallback model if missing

- Shows probability, decision, and lightweight explanations (top n-grams + security features)

- Threshold slider & batch scoring download

- URL table extracted from email



Usage:

1) pip install streamlit scikit-learn joblib pandas numpy transformers torch regex beautifulsoup4 tldextract email_validator

2) Ensure phishing_hitech.py and models/phishing_hitech_v1.joblib are present (or let fallback auto-train a tiny model)

3) streamlit run app_streamlit.py

import os import re import io import json import base64 import pandas as pd import numpy as np import streamlit as st from typing import List, Dict

--- Local high-tech model ---

try: from phishing_hitech import HighTechPhish HAVE_HITECH = True except Exception as e: HAVE_HITECH = False st.warning("Could not import phishing_hitech.HighTechPhish. Fallback model will be used. Error: %s" % e)

--- Minimal fallback (char TF-IDF + LinearSVC calibrated) ---

from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.svm import LinearSVC from sklearn.calibration import CalibratedClassifierCV from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report

import joblib import warnings warnings.filterwarnings("ignore")

URL_RE = re.compile(r'https?://[^\s<>)"]+')

------------------ Helpers ------------------

EXAMPLE_TEXTS = [ "Verify your Microsoft account urgently: http://bad.link/login", "Team meeting at 3 PM. Agenda attached.", "Your package is on hold. Update payment now: http://suspicious.ru/pay", '<html><body>Reset your password <a href="http://secure-update.xyz/reset">here</a></body></html>', ]

def parse_eml(file_bytes: bytes) -> str: try: import email msg = email.message_from_bytes(file_bytes) body_parts = [] if msg.is_multipart(): for part in msg.walk(): ctype = part.get_content_type() if ctype in ("text/plain", "text/html"): try: payload = part.get_payload(decode=True) if payload: body_parts.append(payload.decode(part.get_content_charset() or "utf-8", errors="ignore")) except Exception: pass else: payload = msg.get_payload(decode=True) if payload: body_parts.append(payload.decode(msg.get_content_charset() or "utf-8", errors="ignore")) text = "\n\n".join(body_parts) return text if text.strip() else msg.as_string() except Exception as e: return ""

def extract_urls(text: str) -> List[str]: return URL_RE.findall(text or "")

Fallback model trainer

class FallbackModel: def init(self, threshold: float = 0.5): self.threshold = threshold base = LinearSVC(C=1.0, class_weight="balanced") self.pipe = Pipeline([ ("tfidf", TfidfVectorizer(analyzer="char", ngram_range=(3,5))), ("clf", CalibratedClassifierCV(base, method="sigmoid", cv=3)), ]) self.fitted = False

def fit(self, texts: List[str], labels: List[int]):
    self.pipe.fit(texts, labels)
    self.fitted = True
    return self

def predict_proba(self, texts: List[str]) -> np.ndarray:
    p = self.pipe.predict_proba(texts)[:, 1]
    return p

def predict(self, texts: List[str]) -> np.ndarray:
    p = self.predict_proba(texts)
    return (p >= self.threshold).astype(int)

------------------ UI ------------------

st.set_page_config(page_title="Phishing Detector (High-Tech)", page_icon="🔐", layout="wide") st.title("🔐 Phishing Email Detector — High‑Tech GUI (v3)") st.caption("Hybrid model • Explainability • Batch scoring • .eml support")

with st.sidebar: st.header("⚙️ Settings") threshold = st.slider("Decision Threshold (phishing if ≥)", 0.05, 0.95, 0.35, 0.01) st.write("\nModel status:") model_path = st.text_input("Model path", value="models/phishing_hitech_v1.joblib")

load_clicked = st.button("🔄 Load Model")
train_clicked = st.button("🧪 Train Tiny Fallback (demo)")

Global model object

if 'model_obj' not in st.session_state: st.session_state.model_obj = None st.session_state.is_hitech = False

Load model

if load_clicked: try: if HAVE_HITECH and os.path.exists(model_path): m = HighTechPhish() m.load(model_path) m.threshold = threshold st.session_state.model_obj = m st.session_state.is_hitech = True st.success(f"Loaded High‑Tech model from {model_path}") else: st.error("High‑Tech model not found. Use training fallback or place the joblib file.") except Exception as e: st.exception(e)

Train fallback

if train_clicked or st.session_state.model_obj is None: # Tiny demo dataset data = pd.DataFrame([ {"text": EXAMPLE_TEXTS[0], "label": 1}, {"text": EXAMPLE_TEXTS[1], "label": 0}, {"text": EXAMPLE_TEXTS[2], "label": 1}, {"text": EXAMPLE_TEXTS[3], "label": 1}, {"text": "Invoice for last month attached. Thank you.", "label": 0}, {"text": "Project status: all green. Next review Monday.", "label": 0}, ]) fb = FallbackModel(threshold=threshold) fb.fit(data.text.tolist(), data.label.tolist()) st.session_state.model_obj = fb st.session_state.is_hitech = False st.info("Using tiny fallback model (demo). Load the High‑Tech model from sidebar when ready.")

------------------ Tabs ------------------

Tab = st.tabs(["🔍 Single Check", "📁 Batch CSV", "🧠 About & Tips"])  # type: ignore

---- Single Check ----

with Tab[0]: st.subheader("Analyze a single email") col1, col2 = st.columns([2,1]) with col1: text_input = st.text_area("Paste email text / HTML:", height=260, placeholder="Paste email body here…") up = st.file_uploader("or upload .eml / .txt", type=["eml","txt"], accept_multiple_files=False) if up is not None and not text_input.strip(): if up.type == "text/plain": text_input = up.getvalue().decode("utf-8", errors="ignore") else: text_input = parse_eml(up.getvalue()) ex = st.selectbox("Examples", ["None"] + EXAMPLE_TEXTS) if ex != "None" and not text_input.strip(): text_input = ex with col2: st.markdown("Model") st.write("High‑Tech" if st.session_state.is_hitech else "Fallback (demo)") st.markdown("Threshold") st.write(threshold) st.markdown("Actions") run_single = st.button("🚀 Analyze")

if run_single and text_input.strip():
    model = st.session_state.model_obj
    # set threshold live if model supports it
    if hasattr(model, 'threshold'):
        model.threshold = threshold
    proba = float(model.predict_proba([text_input])[0])
    pred = int(proba >= threshold)

    colA, colB = st.columns([1,1])
    with colA:
        st.metric("Prob (phishing)", f"{proba:.2f}", help="Calibrated probability when available")
        st.metric("Decision", "PHISHING ⚠️" if pred==1 else "LEGIT ✅")
    with colB:
        urls = extract_urls(text_input)
        df_urls = pd.DataFrame({"urls": urls}) if urls else pd.DataFrame({"urls": []})
        st.write("**Extracted URLs**")
        st.dataframe(df_urls, use_container_width=True)

    # Explanations (only for High‑Tech)
    if st.session_state.is_hitech:
        try:
            expl = model.explain([text_input], top_k=8)[0]
            st.markdown("### 🔎 Explanation")
            c1, c2 = st.columns([1,1])
            with c1:
                st.write("**Top char n‑grams (contrib)**")
                if expl.get("top_char_ngrams"):
                    st.dataframe(pd.DataFrame(expl["top_char_ngrams"], columns=["n‑gram","contribution"]))
                else:
                    st.caption("(No n‑gram explanation available)")
            with c2:
                st.write("**Security features**")
                sec_df = pd.DataFrame([expl.get("security_feats", {})]).T
                sec_df.columns = ["value"]
                st.dataframe(sec_df)
            st.write("**Signals**: ",
                     f"urgent_hits={expl.get('urgent_hits')}, brands={', '.join(expl.get('brands_found', []))}")
        except Exception as e:
            st.caption("(Explanation not available)")

---- Batch CSV ----

with Tab[1]: st.subheader("Score a CSV in batch") st.caption("Upload a CSV with a text column. We'll return probability, decision, and URLs found.") csv_up = st.file_uploader("Upload CSV", type=["csv"], accept_multiple_files=False, key="batch") if csv_up is not None: try: df = pd.read_csv(csv_up) assert 'text' in df.columns, "CSV must contain a 'text' column" model = st.session_state.model_obj if hasattr(model, 'threshold'): model.threshold = threshold probs = model.predict_proba(df['text'].astype(str).tolist()) preds = (probs >= threshold).astype(int) df_out = df.copy() df_out['prob_phish'] = probs df_out['prediction'] = np.where(preds==1, 'phishing', 'legit') df_out['num_urls'] = df_out['text'].astype(str).apply(lambda t: len(extract_urls(t))) st.write("Preview") st.dataframe(df_out.head(50), use_container_width=True) # Download link csv_bytes = df_out.to_csv(index=False).encode('utf-8') b64 = base64.b64encode(csv_bytes).decode() st.download_button("⬇️ Download Scored CSV", data=csv_bytes, file_name="scored_emails.csv", mime="text/csv") except Exception as e: st.exception(e)

---- About ----

with Tab[2]: st.markdown(""" ### 🧠 About this app High‑Tech Phishing Detector combines multiple views of the email: - Character n‑grams (robust to obfuscation) - Transformer embeddings (semantic understanding) - Security heuristics (URLs, TLDs, forms, digits, urgency, brand bait)

It uses stacking + probability calibration (when available) and an adjustable threshold to favor **recall** (catch more phishing).

**Tips**
- Lower the threshold (e.g., 0.25–0.35) to reduce misses; expect more false positives.
- Train with **real, diverse samples**; include HTML and obfuscated variants.
- Periodically re‑train and validate; store metrics alongside the model.

**Files expected**
- `phishing_hitech.py` — high‑tech model definition
- `models/phishing_hitech_v1.joblib` — trained weights

Use the sidebar to load the high‑tech model. Until then, a tiny fallback is used for demo.
""")

